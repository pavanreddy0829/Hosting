{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPNBkhuZNXtaf/H3WUZv5qu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavanreddy0829/Hosting/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "7bGOJjU-TQ7F",
        "outputId": "b0c22792-4aaa-4638-a373-a139e16ab447"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# replace the red pixels ( or undesired area ) with\n",
        "# background pixels to generate the invisibility feature.\n",
        "\n",
        "## 1. Hue: This channel encodes color information. Hue can be\n",
        "# thought of an angle where 0 degree corresponds to the red color,\n",
        "# 120 degrees corresponds to the green color, and 240 degrees\n",
        "# corresponds to the blue color.\n",
        "\n",
        "## 2. Saturation: This channel encodes the intensity/purity of color.\n",
        "# For example, pink is less saturated than red.\n",
        "\n",
        "## 3. Value: This channel encodes the brightness of color.\n",
        "# Shading and gloss components of an image appear in this\n",
        "# channel reading the videocapture video\n",
        "\n",
        "# in order to check the cv2 version\n",
        "print(cv2.__version__)\n",
        "\n",
        "# taking video.mp4 as input.\n",
        "# Make your path according to your needs\n",
        "capture_video = cv2.VideoCapture(\"video.mp4\")\n",
        "\t\n",
        "# give the camera to warm up\n",
        "time.sleep(1)\n",
        "count = 0\n",
        "background = 0\n",
        "\n",
        "# capturing the background in range of 60\n",
        "# you should have video that have some seconds\n",
        "# dedicated to background frame so that it\n",
        "# could easily save the background image\n",
        "for i in range(60):\n",
        "\treturn_val, background = capture_video.read()\n",
        "\tif return_val == False :\n",
        "\t\tcontinue\n",
        "\n",
        "background = np.flip(background, axis = 1) # flipping of the frame\n",
        "\n",
        "# we are reading from video\n",
        "while (capture_video.isOpened()):\n",
        "\treturn_val, img = capture_video.read()\n",
        "\tif not return_val :\n",
        "\t\tbreak\n",
        "\tcount = count + 1\n",
        "\timg = np.flip(img, axis = 1)\n",
        "\n",
        "\t# convert the image - BGR to HSV\n",
        "\t# as we focused on detection of red color\n",
        "\n",
        "\t# converting BGR to HSV for better\n",
        "\t# detection or you can convert it to gray\n",
        "\thsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "\t#-------------------------------------BLOCK----------------------------#\n",
        "\t# ranges should be carefully chosen\n",
        "\t# setting the lower and upper range for mask1\n",
        "\tlower_red = np.array([100, 40, 40])\t\n",
        "\tupper_red = np.array([100, 255, 255])\n",
        "\tmask1 = cv2.inRange(hsv, lower_red, upper_red)\n",
        "\t# setting the lower and upper range for mask2\n",
        "\tlower_red = np.array([155, 40, 40])\n",
        "\tupper_red = np.array([180, 255, 255])\n",
        "\tmask2 = cv2.inRange(hsv, lower_red, upper_red)\n",
        "\t#----------------------------------------------------------------------#\n",
        "\n",
        "\t# the above block of code could be replaced with\n",
        "\t# some other code depending upon the color of your cloth\n",
        "\tmask1 = mask1 + mask2\n",
        "\n",
        "\t# Refining the mask corresponding to the detected red color\n",
        "\tmask1 = cv2.morphologyEx(mask1, cv2.MORPH_OPEN, np.ones((3, 3),\n",
        "\t\t\t\t\t\t\t\t\t\tnp.uint8), iterations = 2)\n",
        "\tmask1 = cv2.dilate(mask1, np.ones((3, 3), np.uint8), iterations = 1)\n",
        "\tmask2 = cv2.bitwise_not(mask1)\n",
        "\n",
        "\t# Generating the final output\n",
        "\tres1 = cv2.bitwise_and(background, background, mask = mask1)\n",
        "\tres2 = cv2.bitwise_and(img, img, mask = mask2)\n",
        "\tfinal_output = cv2.addWeighted(res1, 1, res2, 1, 0)\n",
        "\n",
        "\tcv2.imshow(\"INVISIBLE MAN\", final_output)\n",
        "\tk = cv2.waitKey(10)\n",
        "\tif k == 27:\n",
        "\t\tbreak\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.1.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AxisError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1a39ecc611f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mbackground\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flipping of the frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# we are reading from video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mflip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(m, axis)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_axis_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mnormalize_axis_tuple\u001b[0;34m(axis, ndim, argname, allow_duplicate)\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;31m# Going via an iterator directly is slower than via list comprehension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;31m# Going via an iterator directly is slower than via list comprehension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzrV8wGBUVVF"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26C5gEgWUaEJ",
        "outputId": "c7ffe258-bdc3-405f-b4ad-814a5edaf2ba"
      },
      "source": [
        "print(cv2.__version__)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgq1z1zvUhlM"
      },
      "source": [
        "capture_video = cv2.VideoCapture(\"video.mp4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU9lFW5JUn0n"
      },
      "source": [
        "time.sleep(1) \n",
        "count = 0 \n",
        "background = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "e5Fv3Cj2UrOn",
        "outputId": "65b27b3b-7eb0-4af4-8267-07a2dd78d4c4"
      },
      "source": [
        "for i in range(60):\n",
        "    return_val, background = capture_video.read()\n",
        "    if return_val == False :\n",
        "        continue \n",
        "  \n",
        "background = np.flip(background, axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AxisError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c475ef7533b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbackground\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mflip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(m, axis)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_axis_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mnormalize_axis_tuple\u001b[0;34m(axis, ndim, argname, allow_duplicate)\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;31m# Going via an iterator directly is slower than via list comprehension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;31m# Going via an iterator directly is slower than via list comprehension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "8benrmS-VZfD",
        "outputId": "cea9c777-2496-4ccf-ac67-3d1562dd9892"
      },
      "source": [
        "# Python program to play a video\n",
        "# in reverse mode using opencv\n",
        "\n",
        "# import cv2 library\n",
        "import cv2\n",
        "\n",
        "# videoCapture method of cv2 return video object\n",
        "\n",
        "# Pass absolute address of video file\n",
        "cap = cv2.VideoCapture(\"video_file_location\")\n",
        "\n",
        "# read method of video object will return\n",
        "# a tuple with 1st element denotes whether\n",
        "# the frame was read successfully or not,\n",
        "# 2nd element is the actual frame.\n",
        "\n",
        "# Grab the current frame.\n",
        "check , vid = cap.read()\n",
        "\n",
        "# counter variable for\n",
        "# counting frames\n",
        "counter = 0\n",
        "\n",
        "# Initialize the value\n",
        "# of check variable\n",
        "check = True\n",
        "\n",
        "frame_list = []\n",
        "\n",
        "# If reached the end of the video\n",
        "# then we got False value of check.\n",
        "\n",
        "# keep looping until we\n",
        "# got False value of check.\n",
        "while(check == True):\n",
        "\t\n",
        "\t# imwrite method of cv2 saves the\n",
        "\t# image to the specified format.\n",
        "\tcv2.imwrite(\"frame%d.jpg\" %counter , vid)\n",
        "\tcheck , vid = cap.read()\n",
        "\t\n",
        "\t# Add each frame in the list by\n",
        "\t# using append method of the List\n",
        "\tframe_list.append(vid)\n",
        "\t\n",
        "\t# increment the counter by 1\n",
        "\tcounter += 1\n",
        "\n",
        "# last value in the frame_list is None\n",
        "# because when video reaches to the end\n",
        "# then false value store in check variable\n",
        "# and None value is store in vide variable.\n",
        "\n",
        "# removing the last value from the\n",
        "# frame_list by using pop method of List\n",
        "frame_list.pop()\n",
        "\n",
        "# looping in the List of frames.\n",
        "for frame in frame_list:\n",
        "\t\n",
        "\t# show the frame.\n",
        "\tcv2.imshow(\"Frame\" , frame)\n",
        "\t\n",
        "\t# waitkey method to stoping the frame\n",
        "\t# for some time. q key is presses,\n",
        "\t# stop the loop\n",
        "\tif cv2.waitKey(25) and 0xFF == ord(\"q\"):\n",
        "\t\tbreak\n",
        "\t\n",
        "# release method of video\n",
        "# object clean the input video\n",
        "cap.release()\n",
        "\n",
        "# close any open windows\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# reverse the order of the element\n",
        "# present in the list by using\n",
        "# reverse method of the List.\n",
        "frame_list.reverse()\n",
        "\n",
        "for frame in frame_list:\n",
        "\tcv2.imshow(\"Frame\" , frame)\n",
        "\tif cv2.waitKey(25) and 0xFF == ord(\"q\"):\n",
        "\t\tbreak\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1feb6fdc9f4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# imwrite method of cv2 saves the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# image to the specified format.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"frame%d.jpg\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mcheck\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgcodecs/src/loadsave.cpp:715: error: (-215:Assertion failed) !_img.empty() in function 'imwrite'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "nEEQGK3RVsDX",
        "outputId": "51a8ac31-5681-414a-c5b7-845882fe8240"
      },
      "source": [
        "import cv2\n",
        "import imutils\n",
        "\n",
        "# Initializing the HOG person\n",
        "# detector\n",
        "hog = cv2.HOGDescriptor()\n",
        "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
        "\n",
        "# Reading the Image\n",
        "image = cv2.imread('0001.jpg')\n",
        "\n",
        "# Resizing the Image\n",
        "image = imutils.resize(image,\n",
        "\t\t\t\t\twidth=min(400, image.shape[1]))\n",
        "\n",
        "# Detecting all the regions in the\n",
        "# Image that has a pedestrians inside it\n",
        "(regions, _) = hog.detectMultiScale(image,\n",
        "\t\t\t\t\t\t\t\t\twinStride=(4, 4),\n",
        "\t\t\t\t\t\t\t\t\tpadding=(4, 4),\n",
        "\t\t\t\t\t\t\t\t\tscale=1.05)\n",
        "\n",
        "# Drawing the regions in the Image\n",
        "for (x, y, w, h) in regions:\n",
        "\tcv2.rectangle(image, (x, y),\n",
        "\t\t\t\t(x + w, y + h),\n",
        "\t\t\t\t(0, 0, 255), 2)\n",
        "\n",
        "# Showing the output Image\n",
        "cv2.imshow(\"Image\", image)\n",
        "cv2.waitKey(0)\n",
        "\n",
        "cv2.destroyAllWindows()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DisabledFunctionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDisabledFunctionError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-79714a2bffe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Showing the output Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_snippet",
                "actionText": "Search Snippets for cv2.imshow",
                "snippetFilter": "cv2.imshow"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYrMfGiCWGVi"
      },
      "source": [
        "import cv2\n",
        "import imutils\n",
        "\n",
        "# Initializing the HOG person\n",
        "# detector\n",
        "hog = cv2.HOGDescriptor()\n",
        "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
        "\n",
        "cap = cv2.VideoCapture('vid.mp4')\n",
        "\n",
        "while cap.isOpened():\n",
        "\t# Reading the video stream\n",
        "\tret, image = cap.read()\n",
        "\tif ret:\n",
        "\t\timage = imutils.resize(image,\n",
        "\t\t\t\t\t\t\twidth=min(400, image.shape[1]))\n",
        "\n",
        "\t\t# Detecting all the regions\n",
        "\t\t# in the Image that has a\n",
        "\t\t# pedestrians inside it\n",
        "\t\t(regions, _) = hog.detectMultiScale(image,\n",
        "\t\t\t\t\t\t\t\t\t\t\twinStride=(4, 4),\n",
        "\t\t\t\t\t\t\t\t\t\t\tpadding=(4, 4),\n",
        "\t\t\t\t\t\t\t\t\t\t\tscale=1.05)\n",
        "\n",
        "\t\t# Drawing the regions in the\n",
        "\t\t# Image\n",
        "\t\tfor (x, y, w, h) in regions:\n",
        "\t\t\tcv2.rectangle(image, (x, y),\n",
        "\t\t\t\t\t\t(x + w, y + h),\n",
        "\t\t\t\t\t\t(0, 0, 255), 2)\n",
        "\n",
        "\t\t# Showing the output Image\n",
        "\t\tcv2.imshow(\"Image\", image)\n",
        "\t\tif cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "\t\t\tbreak\n",
        "\telse:\n",
        "\t\tbreak\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8XspdhOX8gB"
      },
      "source": [
        "import cv2\n",
        "\n",
        "# path =\"C:/Users/Personal/Downloads/black dot.jpg\"\n",
        "path =\"black-dot1.jpg\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9ZMn3TfYDCg"
      },
      "source": [
        "gray = cv2.imread(path, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TYZIPjeYF04"
      },
      "source": [
        "# threshold\n",
        "th, threshed = cv2.threshold(gray, 100, 255,\n",
        "\tcv2.THRESH_BINARY_INV|cv2.THRESH_OTSU)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2roYZHlYIF4"
      },
      "source": [
        "# findcontours\n",
        "cnts = cv2.findContours(threshed, cv2.RETR_LIST,\n",
        "\t\t\t\t\tcv2.CHAIN_APPROX_SIMPLE)[-2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNtdMiWOYJ0J"
      },
      "source": [
        "# filter by area\n",
        "s1 = 3\n",
        "s2 = 20\n",
        "xcnts = []\n",
        "for cnt in cnts:\n",
        "\tif s1<cv2.contourArea(cnt) <s2:\n",
        "\t\txcnts.append(cnt)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdseXJS_YM3S",
        "outputId": "16d77b29-5829-4799-bc11-3da78da35cbe"
      },
      "source": [
        "print(\"\\nDots number: {}\".format(len(xcnts)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dots number: 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G3gTxORYjVD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b85c6d34-f6f6-4d18-dccc-384ead64e0c8"
      },
      "source": [
        "import cv2\n",
        "path =\"white-dot.png\"\n",
        "\n",
        "# reading the image in grayscale mode\n",
        "gray = cv2.imread(path, 0)\n",
        "\n",
        "# threshold\n",
        "th, threshed = cv2.threshold(gray, 100, 255,\n",
        "\t\tcv2.THRESH_BINARY|cv2.THRESH_OTSU)\n",
        "\n",
        "# findcontours\n",
        "cnts = cv2.findContours(threshed, cv2.RETR_LIST,\n",
        "\t\t\t\t\tcv2.CHAIN_APPROX_SIMPLE)[-2]\n",
        "\n",
        "# filter by area\n",
        "s1 = 3\n",
        "s2 = 20\n",
        "xcnts = []\n",
        "\n",
        "for cnt in cnts:\n",
        "\tif s1<cv2.contourArea(cnt) <s2:\n",
        "\t\txcnts.append(cnt)\n",
        "\n",
        "# printing output\n",
        "print(\"\\nDots number: {}\".format(len(xcnts)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dots number: 583\n"
          ]
        }
      ]
    }
  ]
}